{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM for Programmers",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxmv1EHupj5P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNGg3w0ApQYZ",
        "colab_type": "text"
      },
      "source": [
        "## LSTM for Programmers\n",
        "\n",
        "First we define some parameters:\n",
        "\n",
        "- `seq_len` (Sequence length): The length of sequence. For RNN, it can vary\n",
        "  *from batch to batch*. As a first step, we will define it as a constant and\n",
        "  show how LSTM works in a single batch. We also call this timestep length, or\n",
        "  timesteps.\n",
        "- `input_size` (Input feature size): For each input, e.g. a word in a sentence,\n",
        "  we use a vector of `input_size`-long to represent it. We normally call this\n",
        "  the *embedding* of the actual individual data (word).\n",
        "- `batch_size` (Batch size): how many copies of data in a single batch\n",
        "- `hidden_size` (Hidden units size): the number of features in hidden state.\n",
        "  Note that hidden state is the *output* features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBB4SpM3r2fW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_len, batch_size, input_size, hidden_size = 17, 2, 10, 7\n",
        "\n",
        "I_tc = torch.randn(seq_len, batch_size, input_size) # Input for PyTorch\n",
        "I_tf = tf.random.normal([batch_size, seq_len, input_size]) # Input for TensorFlow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYXzA2kfumdB",
        "colab_type": "text"
      },
      "source": [
        "## Simple LSTMs\n",
        "\n",
        "Now, let's show a simple [LSTM layer in PyTorch](https://pytorch.org/docs/stable/nn.html#lstm) first:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Do0bEClDpjqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_tc_simple = torch.nn.LSTM(input_size, hidden_size)\n",
        "O_tc_simple, (H_tc_simple, C_tc_simple) = lstm_tc_simple(I_tc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnoUv0m0q9ge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert O_tc_simple.shape == torch.Size((seq_len, batch_size, hidden_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mulidEJkrEt2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert H_tc_simple.shape == torch.Size((1, batch_size, hidden_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC4_ZV-1rFhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert C_tc_simple.shape == torch.Size((1, batch_size, hidden_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuEEVpLfsr9-",
        "colab_type": "text"
      },
      "source": [
        "Next, another example in [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUSytTEUsaEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_tf_simple = tf.keras.layers.LSTM(hidden_size)\n",
        "O_tf_simple = lstm_tf_simple(I_tf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q8x4n_PtWP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert O_tf_simple.shape == tf.TensorShape((batch_size, hidden_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swiRMn8fwoNR",
        "colab_type": "text"
      },
      "source": [
        "As you can see, the input of two `lstm`s are identical (modulo the order).\n",
        "But, there are several questions we can ask about the *difference* between the\n",
        "outputs:\n",
        "\n",
        "- What does `O_tc_simple`, `H_tc_simple` and `C_tc_simple` mean?\n",
        "- What does `O_tf_simple` mean? How does it correspond to the PyTorch gang?\n",
        "\n",
        "Let's look at some diagrams from [Colah's post on LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/):\n",
        "\n",
        "![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)\n",
        "\n",
        "The above is a general structure of RNN:\n",
        "\n",
        "> In the above diagram, a chunk of neural network, $A$, looks at some input $x_t$ and outputs a value $h_t$. A loop allows information to be passed from one step of the network to the next.\n",
        "\n",
        "So, LSTM (or any general RNN)'s basic functionality is converting\n",
        "a *sequence* of $x$ to a *sequence* of $h$.\n",
        "\n",
        "`O_tc_simple` contains a *sequence* of hidden state (or output features) $h$\n",
        "multiplied with batch size.\n",
        "\n",
        "`H_tc_simple` contains the hidden state for most recent timestep,\n",
        "in shape of `(num_layers * num_directions, batch_size, hidden_size)`.\n",
        "Since by default we constructed LSTM of only one layer and one direction,\n",
        "the first dimension is just 1.\n",
        "\n",
        "Similar to `H_tc_simple`'s shape,\n",
        "`C_tc_simple` contains the *cell* state. You can think of\n",
        "each hidden state is associated with a cell state $c$:\n",
        "\n",
        "![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png)\n",
        "\n",
        "In fact, we can verify that when there is only one layer, `O_tc_simple` contains `H_tc_simple`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDleKMt03WhD",
        "colab_type": "code",
        "outputId": "086f5b6a-86a2-4990-f2f7-c72f9be193d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "torch.eq(O_tc_simple[-1,:,:], H_tc_simple)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[True, True, True, True, True, True, True],\n",
              "         [True, True, True, True, True, True, True]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2bRo_RN37pm",
        "colab_type": "text"
      },
      "source": [
        "Tensorflow's `O_tf_simple` is actuall just the flatten version of\n",
        "`H_tc_simple`. There is another\n",
        "configuration of TF's LSTM to make it output differently:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXesqv6G3dBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_tf_simple2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True, return_state=True)\n",
        "O_tf_simple2, H_tf_simple2, C_tf_simple2 = lstm_tf_simple2(I_tf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma9lvsJM5dJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert O_tf_simple2.shape == tf.TensorShape((batch_size, seq_len, hidden_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWPpZJOm5Jbb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert H_tf_simple2.shape == tf.TensorShape((batch_size, hidden_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSicXaRk5Yps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert C_tf_simple2.shape == tf.TensorShape((batch_size, hidden_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I7nvuog5nx9",
        "colab_type": "text"
      },
      "source": [
        "Now, correspondence between PyTorch's and TF's outputs is much clearer.\n",
        "\n",
        "## Bidirectional and Stacked LSTMs\n",
        "\n",
        "However, it looks like PyTorch's `LSTM` is much more powerful. You can specify\n",
        "two directions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pycKMDMP5bqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_directions = 2\n",
        "bilstm_tc = torch.nn.LSTM(input_size, hidden_size, bidirectional=True)\n",
        "O_tc_bidir, (H_tc_bidir, C_tc_bidir) = bilstm_tc(I_tc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfmnO98s6X7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert O_tc_bidir.shape == torch.Size((seq_len, batch_size, num_directions * hidden_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j15Wep_Q6eFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert H_tc_bidir.shape == torch.Size((num_directions, batch_size, hidden_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kbIUszBCCDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert C_tc_bidir.shape == torch.Size((num_directions, batch_size, hidden_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QZiUJ6L-L3Z",
        "colab_type": "text"
      },
      "source": [
        "Or stack multiple layers together:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i4Usn9WCWq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_layers = 3\n",
        "bi_lstm_tc_stacked = torch.nn.LSTM(input_size, hidden_size, num_layers=num_layers, bidirectional=True)\n",
        "O_tc_bidir_stacked, (H_tc_bidir_stacked, C_tc_bidir_stacked) = bi_lstm_tc_stacked(I_tc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WO_3KldnCdks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert O_tc_bidir_stacked.shape == torch.Size((seq_len, batch_size, num_directions * hidden_size)), O1.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV7a8QJpChlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert H_tc_bidir_stacked.shape == torch.Size((num_directions * num_layers, batch_size, hidden_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y3itfN_CiS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert C_tc_bidir_stacked.shape == torch.Size((num_directions * num_layers, batch_size, hidden_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBSJYLepCvhz",
        "colab_type": "text"
      },
      "source": [
        "How to do the same extension in TF?\n",
        "\n",
        "[Bidirectional](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional) is a wrapper over an RNN layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYk0tgonCzA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bilstm_tf = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(hidden_size), input_shape=I_tf.shape)\n",
        "O_tf_bidir = bilstm_tf(I_tf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQX1UVRV3sM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert O_tf_bidir.shape == tf.TensorShape((batch_size, 2 * hidden_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEggXeuT-c8b",
        "colab_type": "text"
      },
      "source": [
        "Stacking LSTM layers in TF requires composing them in a sequential model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqTVLIjd5X0n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "a18315d5-d89e-4a75-931a-9620feacecfe"
      },
      "source": [
        "# Stack 3 layers\n",
        "\n",
        "model_tf_stacked = tf.keras.Sequential()\n",
        "model_tf_stacked.add(tf.keras.layers.LSTM(hidden_size, return_sequences=True, input_shape=(seq_len, input_size)))\n",
        "model_tf_stacked.add(tf.keras.layers.LSTM(hidden_size, return_sequences=True))\n",
        "model_tf_stacked.add(tf.keras.layers.LSTM(hidden_size))\n",
        "model_tf_stacked.compile(optimizer='adam', loss='mse')\n",
        "model_tf_stacked.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_5 (LSTM)                (None, 17, 7)             504       \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 17, 7)             420       \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 7)                 420       \n",
            "=================================================================\n",
            "Total params: 1,344\n",
            "Trainable params: 1,344\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k2Cmb0E7QTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "O_tf_stacked = model_tf_stacked.predict(I_tf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gu2atfK7fKe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert O_tf_stacked.shape == tf.TensorShape((batch_size, hidden_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfKEIRGK-7zC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}