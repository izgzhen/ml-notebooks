{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aggregation NN for Programmers",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxmv1EHupj5P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import tensorflow as tf\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNGg3w0ApQYZ",
        "colab_type": "text"
      },
      "source": [
        "## Aggregation NN for Programmers\n",
        "\n",
        "Aggregation NN refers to use of several types of NN\n",
        "e.g. CNN, RNN etc. for *aggregation* purpose, on a high level that is,\n",
        "aggregating many inputs into a smaller set of summary outputs (sometimes just one output)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYXzA2kfumdB",
        "colab_type": "text"
      },
      "source": [
        "## Convolutional Layers\n",
        "\n",
        "Question: What is the difference/relationship between\n",
        "conv layers and pooling layers?\n",
        "\n",
        "[Conv2d in PyTorch](https://pytorch.org/docs/master/generated/torch.nn.Conv2d.html#torch.nn.Conv2d):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Do0bEClDpjqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 20\n",
        "height = 50\n",
        "width = 100\n",
        "in_channels = 16\n",
        "out_channels = 33\n",
        "\n",
        "# assuming square\n",
        "kernel_size = 3\n",
        "stride = 1 # default\n",
        "padding = 0 # default\n",
        "dilation = 1 # default\n",
        "\n",
        "height_out = math.floor((height + 2 * padding - dilation * (kernel_size - 1) - 1) / stride + 1)\n",
        "width_out = math.floor((width + 2 * padding - dilation * (kernel_size - 1) - 1) / stride + 1)\n",
        "\n",
        "cond2d = torch.nn.Conv2d(in_channels=in_channels,\n",
        "                         out_channels=out_channels,\n",
        "                         kernel_size=kernel_size)\n",
        "I = torch.randn(batch_size, in_channels, height, width)\n",
        "O = cond2d(I)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnoUv0m0q9ge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert O.shape == torch.Size((batch_size, out_channels, height_out, width_out))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2b6k9QPplPD",
        "colab_type": "text"
      },
      "source": [
        "## Pooling Layers\n",
        "\n",
        "[MaxPool2d in PyTorch](https://pytorch.org/docs/stable/nn.html#maxpool2d)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5S3ASEbmTdM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 20\n",
        "height = 50\n",
        "width = 100\n",
        "channels = 16\n",
        "\n",
        "kernel_size = 3\n",
        "stride = kernel_size # default\n",
        "padding = 0 # default\n",
        "dilation = 1 # default\n",
        "\n",
        "# The same formula as the Conv2d\n",
        "height_out = math.floor((height + 2 * padding - dilation * (kernel_size - 1) - 1) / stride + 1)\n",
        "width_out = math.floor((width + 2 * padding - dilation * (kernel_size - 1) - 1) / stride + 1)\n",
        "\n",
        "maxpool2d = torch.nn.MaxPool2d(kernel_size)\n",
        "I = torch.randn(batch_size, channels, height, width)\n",
        "O = maxpool2d(I)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OX3VOqcqZ7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert O.shape == torch.Size((batch_size, channels, height_out, width_out))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhfd8CjcrQ9Z",
        "colab_type": "text"
      },
      "source": [
        "## Recurrent Layers\n",
        "\n",
        "TODO: use_attention\n",
        "\n",
        "TODO: compare with LSTM (and the post)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMNZYVBAqbwC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_size = 10\n",
        "hidden_size = 20\n",
        "seq_len, batch_size = 5, 3\n",
        "\n",
        "rnn = torch.nn.RNN(input_size, hidden_size)\n",
        "I = torch.randn(seq_len, batch_size, input_size)\n",
        "O, H = rnn(I)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWIQhZyqsUTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert O.shape == torch.Size((seq_len, batch_size, hidden_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJ7uhz9ssUwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert H.shape == torch.Size((1, batch_size, hidden_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X6naJaqs7N7",
        "colab_type": "text"
      },
      "source": [
        "## Attention\n",
        "\n",
        "Attention can be complex. But understanding it from a\n",
        "simpler perspective, it is similar to min/max/mean --\n",
        "we use it to aggregate a sequence of embeddings into\n",
        "one.\n",
        "\n",
        "Attention itself is not a neural network primitive. It can be implemented from simpler ones. Here, we present\n",
        "a simplifier version of attention layer based on the\n",
        "[attention decoder in PyTorch tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#attention-decoder).\n",
        "\n",
        "Note how embedding is used (see *insert link* for tutorial\n",
        "on using embedding layer). Note that the most important thing\n",
        "is that embedding embeds each index as an embedding vector,\n",
        "and this API is universal for all implementations, including\n",
        "BERT's fine-tuning interface."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbfFa7IFslV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 256\n",
        "hidden_size = embedding_dim\n",
        "\n",
        "max_length = 10\n",
        "num_embeddings = 20\n",
        "\n",
        "# TODO: explain why they are both one in this case\n",
        "batch_size = 1\n",
        "seq_len    = 1\n",
        "\n",
        "min_index = 0\n",
        "max_index = min_index + num_embeddings\n",
        "\n",
        "attn = torch.nn.Linear(hidden_size * 2, max_length)\n",
        "attn_combine = torch.nn.Linear(hidden_size * 2, hidden_size)\n",
        "\n",
        "I = torch.randint(min_index, max_index, (batch_size, seq_len))\n",
        "\n",
        "# Prepare inputs \n",
        "embedding = torch.nn.Embedding(num_embeddings, hidden_size)\n",
        "embedded = embedding(I)\n",
        "assert embedded.shape == torch.Size((batch_size, seq_len, embedding_dim)), embedded.shape\n",
        "embedded = embedded.view(1, 1, -1)\n",
        "assert embedded.shape == torch.Size((1, 1, batch_size * seq_len * embedding_dim))\n",
        "assert embedded[0].shape == torch.Size((1, batch_size * seq_len * embedding_dim))\n",
        "\n",
        "hidden = torch.zeros(1, 1, hidden_size)\n",
        "assert hidden[0].shape == torch.Size((1, hidden_size))\n",
        "\n",
        "embedded_hidden = torch.cat((embedded[0], hidden[0]), dim=1)\n",
        "assert embedded_hidden.shape == torch.Size((1, hidden_size + batch_size * seq_len * embedding_dim))\n",
        "assert embedded_hidden.shape == torch.Size((1, 2 * hidden_size))\n",
        "\n",
        "encoder_outputs = torch.zeros(max_length, hidden_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfUpdZG_7ol7",
        "colab_type": "text"
      },
      "source": [
        "Now, we first compute the attention weights from `embedded_hidden`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNRRbjBc7nQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "O = attn(embedded_hidden)\n",
        "assert O.shape == torch.Size((1, max_length))\n",
        "attn_weights = torch.nn.functional.softmax(O, dim=1)\n",
        "assert attn_weights.shape == torch.Size((1, max_length))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFD_zh7T7w_M",
        "colab_type": "text"
      },
      "source": [
        "Then we can apply the weights on incoming encoder outputs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUJS4dozyPgg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
        "assert attn_applied.shape == torch.Size((1, 1, embedding_dim))"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBNSNh1974ZL",
        "colab_type": "text"
      },
      "source": [
        "Essentially, attention layer summarize `max_length` many encoder outputs as a single one. You can read the entire PyTorch tutorial to understand the context\n",
        "of this many-to-one mapping.\n",
        "\n",
        "![](https://pytorch.org/tutorials/_images/attention-decoder-network.png)"
      ]
    }
  ]
}